{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "These instructions explain how to get CUDA working on WSL2 and installing LLamaCpp in WSL with CUDA acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install CUDA on Windows\n",
    "Install Cuda https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Install CUDA in WSL2\n",
    "https://ubuntu.com/tutorials/enabling-gpu-acceleration-on-ubuntu-on-wsl2-with-the-nvidia-cuda-platform#4-compile-a-sample-application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install llamacpp with CUDA blas support\n",
    "\n",
    "First optionally setup and activate a python or conda environment.\n",
    "\n",
    "Instructions here\n",
    "https://github.com/abetlen/llama-cpp-python.git\n",
    "\n",
    "Command to install llamacpp with CUDA blas support\n",
    "```\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --no-cache-dir\n",
    "```\n",
    "\n",
    "To resinstall the package (to fix nvidia support for example) use the following command\n",
    "```\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server] --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "```\n",
    "\n",
    "Can check for cuda (cblas) support with the following command\n",
    "```\n",
    "python -c \"from llama_cpp import GGML_USE_CUBLAS; print(GGML_USE_CUBLAS)\"\n",
    "```\n",
    "\n",
    "If not working very good troubleshooting steps here\n",
    "https://github.com/abetlen/llama-cpp-python/issues/880\n",
    "\n",
    "and here\n",
    "\n",
    "https://github.com/abetlen/llama-cpp-python/issues/627"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIP not finding CUDA\n",
    "Initially when running the pip install command with the --verbose command I found that it couldnt identify cuda\n",
    "```\n",
    "-- Unable to find cudart library.\n",
    "  -- Could NOT find CUDAToolkit (missing: CUDA_CUDART) (found version \"12.3.52\")\n",
    "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:305 (message):\n",
    "    cuBLAS not found\n",
    "```\n",
    "\n",
    "I ended up needing to run the following command to point to the correct cuda libraries\n",
    "```\n",
    "CUDACXX=/usr/local/cuda-12.3/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.3 -DCUDAToolkit_ROOT=/usr/local/cuda-12.3 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12.3/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.3/lib64 -DCMAKE_CUDA_ARCHITECTURES=native\" FORCE_CMAKE=1  pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Langchain\n",
    "```\n",
    "pip install langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
