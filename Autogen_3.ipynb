{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is part 3 of using autogen with local ai models to query an arbitrary database.\n",
    "In this notebook I will use autogen with ChatGPT to connect to a local database and query it.  I will not be using local AI models\n",
    "I'm using this reference https://github.com/disler/multi-agent-postgres-data-analytics/tree/v1-prompt-engineering-an-entire-codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in ./.venv/lib/python3.10/site-packages (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai==0.28.1\n",
      "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting requests>=2.20 (from openai==0.28.1)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from openai==0.28.1)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohttp (from openai==0.28.1)\n",
      "  Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.20->openai==0.28.1)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai==0.28.1)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai==0.28.1)\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai==0.28.1)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.3\n",
      "    Uninstalling async-timeout-4.0.3:\n",
      "      Successfully uninstalled async-timeout-4.0.3\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.4\n",
      "    Uninstalling yarl-1.9.4:\n",
      "      Successfully uninstalled yarl-1.9.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.1\n",
      "    Uninstalling aiohttp-3.9.1:\n",
      "      Successfully uninstalled aiohttp-3.9.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.11.17 charset-normalizer-3.3.2 frozenlist-1.4.0 idna-3.6 multidict-6.0.4 openai-0.28.1 requests-2.31.0 tqdm-4.66.1 urllib3-2.1.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyodbc\n",
    "%pip install python-dotenv\n",
    "%pip install pyautogen\n",
    "%pip install --upgrade --force-reinstall openai==0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the required environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create code to handle connecting to the database\n",
    "The settings for the database should obviously be in a config file, but for now I'm just going to hard code them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MSSQL database\n",
    "import pyodbc\n",
    "\n",
    "SERVER = '127.0.0.1'\n",
    "DATABASE = 'TimeBasedCommitments'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = 'BadDefaultPassword!'\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD}'\n",
    "\n",
    "conn = pyodbc.connect(connectionString)\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all table definitions\n",
    "The table information will be passed to the AI prompt so it can create queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table names\n",
    "get_all_tables_stmt = \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES;\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(get_all_tables_stmt)\n",
    "table_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Get all table definitions and format them into CREATE TABLE statements\n",
    "get_def_stmt = f\"\"\"\n",
    "SELECT COLUMN_NAME, DATA_TYPE\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE TABLE_NAME = ?\n",
    "\"\"\"\n",
    "\n",
    "definitions = []\n",
    "for table_name in table_names:\n",
    "    cursor.execute(get_def_stmt, (table_name,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    create_table_stmt = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    for row in rows:\n",
    "        create_table_stmt += f\"{row[0]} {row[1]},\\n\"\n",
    "    create_table_stmt = create_table_stmt.rstrip(\",\\n\") + \"\\n);\"\n",
    "    \n",
    "    definitions.append(create_table_stmt)\n",
    "\n",
    "table_definitions = \"\\n\\n\".join(definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that agent can use to run SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql(sql):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    print(f\"\\n\\n-------- SQL QUERY RESULTS --------\")\n",
    "    print(rows)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "# build the gpt_configuration object\n",
    "gpt4_config = {\n",
    "    \"use_cache\": False,\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": autogen.config_list_from_models([\"gpt-4\"]),\n",
    "    \"request_timeout\": 120,\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"run_sql\",\n",
    "            \"description\": \"Run a SQL query against the postgres database\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"sql\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The SQL query to run\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"sql\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# build the function map to make functions available to some autogen agents\n",
    "function_map = {\n",
    "    \"run_sql\": run_sql,\n",
    "}\n",
    "\n",
    "# create our terminate msg function\n",
    "def is_termination_msg(content):\n",
    "    have_content = content.get(\"content\", None) is not None\n",
    "    if have_content and \"APPROVED\" in content[\"content\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "COMPLETION_PROMPT = \"If everything looks good, respond with APPROVED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROXY_PROMPT = (\n",
    "    \"A human admin. Interact with the Product Manager to discuss the plan. Plan execution needs to be approved by this admin.\"\n",
    "    + COMPLETION_PROMPT\n",
    ")\n",
    "DATA_ENGINEER_PROMPT = (\n",
    "    \"A Data Engineer. You follow an approved plan. Generate the initial SQL based on the requirements provided. Send it to the Sr Data Analyst to be executed.\"\n",
    "    + COMPLETION_PROMPT\n",
    ")\n",
    "SR_DATA_ANALYST_PROMPT = (\n",
    "    \"Sr Data Analyst. You follow an approved plan. You run the SQL query, generate the response and send it to the product manager for final review.\"\n",
    "    + COMPLETION_PROMPT\n",
    ")\n",
    "PRODUCT_MANAGER_PROMPT = (\n",
    "    \"Product Manager. Validate the response to make sure it's correct\"\n",
    "    + COMPLETION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the agent roles/settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of agents with specific roles\n",
    "# admin user proxy agent - takes in the prompt and manages the group chat\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=USER_PROXY_PROMPT,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=is_termination_msg,\n",
    ")\n",
    "\n",
    "# data engineer agent - generates the sql query\n",
    "data_engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=DATA_ENGINEER_PROMPT,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=is_termination_msg,\n",
    ")\n",
    "\n",
    "# sr data analyst agent - run the sql query and generate the response\n",
    "sr_data_analyst = autogen.AssistantAgent(\n",
    "    name=\"Sr_Data_Analyst\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=SR_DATA_ANALYST_PROMPT,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=is_termination_msg,\n",
    "    function_map=function_map,\n",
    ")\n",
    "\n",
    "# product manager - validate the response to make sure it's correct\n",
    "product_manager = autogen.AssistantAgent(\n",
    "    name=\"Product_Manager\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=PRODUCT_MANAGER_PROMPT,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=is_termination_msg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the group chat and add the agents to the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, data_engineer, sr_data_analyst, product_manager],\n",
    "    messages=[],\n",
    "    max_round=10,\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the group and initiate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Fulfill this database query: how many users are in the database?.  Use these TABLE_DEFINITIONS to satisfy the database query.\n",
      "\n",
      "TABLE_DEFINITIONS\n",
      "\n",
      "CREATE TABLE tblAudit (\n",
      "id bigint,\n",
      "date datetime,\n",
      "user_id bigint,\n",
      "logdata nvarchar\n",
      ");\n",
      "\n",
      "CREATE TABLE tblCommitments (\n",
      "id bigint,\n",
      "active bit,\n",
      "title varchar,\n",
      "description varchar,\n",
      "added datetime,\n",
      "modified datetime,\n",
      "instructions varchar,\n",
      "frequency varchar,\n",
      "lookahead int,\n",
      "length int\n",
      ");\n",
      "\n",
      "CREATE TABLE tblServices (\n",
      "id bigint,\n",
      "service_name varchar,\n",
      "version varchar,\n",
      "last_connected datetime,\n",
      "error_message varchar,\n",
      "last_error datetime,\n",
      "healthy bit\n",
      ");\n",
      "\n",
      "CREATE TABLE tblReports (\n",
      "id int,\n",
      "label varchar,\n",
      "config nvarchar\n",
      ");\n",
      "\n",
      "CREATE TABLE tblReportSchedules (\n",
      "id bigint,\n",
      "schedule varchar,\n",
      "report_name varchar,\n",
      "last_sent datetime\n",
      ");\n",
      "\n",
      "CREATE TABLE tblResponsibleToCommitments (\n",
      "id bigint,\n",
      "commitment_id bigint,\n",
      "user_id bigint\n",
      ");\n",
      "\n",
      "CREATE TABLE tblSources (\n",
      "id bigint,\n",
      "description varchar,\n",
      "active bit\n",
      ");\n",
      "\n",
      "CREATE TABLE tblSourcesToCommitments (\n",
      "id bigint,\n",
      "commitment_id bigint,\n",
      "source_id bigint\n",
      ");\n",
      "\n",
      "CREATE TABLE tblSpecificCommitments (\n",
      "id bigint,\n",
      "commitment_id bigint,\n",
      "date datetime,\n",
      "due datetime,\n",
      "completed datetime,\n",
      "completed_by bigint,\n",
      "verified datetime,\n",
      "verified_by bigint,\n",
      "closed datetime,\n",
      "closed_by bigint,\n",
      "title varchar,\n",
      "notes varchar,\n",
      "preserve bit\n",
      ");\n",
      "\n",
      "CREATE TABLE tblUsers (\n",
      "id bigint,\n",
      "email varchar,\n",
      "password varchar,\n",
      "otp varchar,\n",
      "firstname varchar,\n",
      "lastname varchar,\n",
      "admin bit,\n",
      "active bit,\n",
      "passwordreset bit,\n",
      "otpreset bit\n",
      ");\n",
      "\n",
      "CREATE TABLE tblScheduleCodes (\n",
      "id bigint,\n",
      "code char,\n",
      "label varchar,\n",
      "val_description varchar,\n",
      "val_min int,\n",
      "val_max int\n",
      ");\n",
      "\n",
      "CREATE TABLE tblUsersToReportSchedules (\n",
      "id bigint,\n",
      "report_schedule_id bigint,\n",
      "user_id bigint\n",
      ");\n",
      "\n",
      "CREATE TABLE tblEmailSettings (\n",
      "lock char,\n",
      "address varchar,\n",
      "port int,\n",
      "tls bit,\n",
      "username varchar,\n",
      "password varchar,\n",
      "sender varchar,\n",
      "limit int\n",
      ");\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "APPROVED\n",
      "\n",
      "SELECT COUNT(*) FROM tblUsers;\n",
      "\n",
      "Result: 10\n",
      "\n",
      "Note: This is just an example response. The actual result will depend on the data in your database.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "user_question = \"how many users are in the database?\"\n",
    "\n",
    "prompt = f\"Fulfill this database query: {user_question}. \"\n",
    "\n",
    "# add the database info to the prompt\n",
    "POSTGRES_TABLE_DEFINITIONS_CAP_REF = \"TABLE_DEFINITIONS\"\n",
    "prompt_suffix = f\"Use these {POSTGRES_TABLE_DEFINITIONS_CAP_REF} to satisfy the database query.\"\n",
    "\n",
    "prompt = f\"\"\"{prompt} {prompt_suffix}\\n\\n{POSTGRES_TABLE_DEFINITIONS_CAP_REF}\\n\\n{table_definitions}\"\"\"\n",
    "\n",
    "# initiate the chat\n",
    "result = user_proxy.initiate_chat(manager, clear_history=True, message=prompt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.openai.com\n"
     ]
    }
   ],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
