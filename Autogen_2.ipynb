{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is part 2 of using autogen with local ai models to query an arbitrary database.\n",
    "In this notebook I will connect a local AI model to a local database and query it.  I will not be using autogen. This will be the same as example 1 but instead of chatGPT I will use a local model and I will run from windows instead of linux.\n",
    "I'm using this reference https://github.com/disler/multi-agent-postgres-data-analytics/tree/v1-prompt-engineering-an-entire-codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install ODBC Driver for MSSQL\n",
    "For this example I did not need to install any additional ODBC drivers in windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in ./.venv/lib/python3.10/site-packages (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai==0.28.1\n",
      "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting requests>=2.20 (from openai==0.28.1)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from openai==0.28.1)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohttp (from openai==0.28.1)\n",
      "  Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.20->openai==0.28.1)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.20->openai==0.28.1)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai==0.28.1)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai==0.28.1)\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai==0.28.1)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->openai==0.28.1)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.3\n",
      "    Uninstalling async-timeout-4.0.3:\n",
      "      Successfully uninstalled async-timeout-4.0.3\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.4\n",
      "    Uninstalling yarl-1.9.4:\n",
      "      Successfully uninstalled yarl-1.9.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.1\n",
      "    Uninstalling aiohttp-3.9.1:\n",
      "      Successfully uninstalled aiohttp-3.9.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.11.17 charset-normalizer-3.3.2 frozenlist-1.4.0 idna-3.6 multidict-6.0.4 openai-0.28.1 requests-2.31.0 tqdm-4.66.1 urllib3-2.1.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyodbc\n",
    "%pip install python-dotenv\n",
    "%pip install --upgrade --force-reinstall openai==0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create code to handle connecting to the database\n",
    "The settings for the database should obviously be in a config file, but for now I'm just going to hard code them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MSSQL database\n",
    "import pyodbc\n",
    "\n",
    "SERVER = '127.0.0.1'\n",
    "DATABASE = 'TimeBasedCommitments'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = 'BadDefaultPassword!'\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD}'\n",
    "\n",
    "conn = pyodbc.connect(connectionString)\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all table definitions\n",
    "The table information will be passed to the AI prompt so it can create queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table names\n",
    "get_all_tables_stmt = \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES;\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(get_all_tables_stmt)\n",
    "table_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Get all table definitions and format them into CREATE TABLE statements\n",
    "get_def_stmt = f\"\"\"\n",
    "SELECT COLUMN_NAME, DATA_TYPE\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE TABLE_NAME = ?\n",
    "\"\"\"\n",
    "\n",
    "definitions = []\n",
    "for table_name in table_names:\n",
    "    cursor.execute(get_def_stmt, (table_name,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    create_table_stmt = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    for row in rows:\n",
    "        create_table_stmt += f\"{row[0]} {row[1]},\\n\"\n",
    "    create_table_stmt = create_table_stmt.rstrip(\",\\n\") + \"\\n);\"\n",
    "    \n",
    "    definitions.append(create_table_stmt)\n",
    "\n",
    "table_definitions = \"\\n\\n\".join(definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the AI model\n",
    "Sets api base to point at local model instead of ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the prompt\n",
    "I tried using the same prompt as GPT4 but it wasnt working because llama2 wasnt respecting the response format.  Each time it would put the query in a different place and wasnt using the delimiter.  Instead I made the request simpler and ask for ONLY the SQL query.  This may be fixable when I am using autogen and the llm models can deal with eachothers responses to pull out the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the user input\n",
    "user_question = \"How many users are there in the database?\"\n",
    "\n",
    "# The rest of this prompt building is back end logic\n",
    "## Build the request\n",
    "prompt = f\"Create a SQL query that can be used to answer this question: {user_question}. \"\n",
    "POSTGRES_TABLE_DEFINITIONS_CAP_REF = \"TABLE_DEFINITIONS\"\n",
    "prompt_suffix = f\"Use these {POSTGRES_TABLE_DEFINITIONS_CAP_REF} to satisfy the database query.\"\n",
    "\n",
    "prompt = f\"\"\"{prompt} {prompt_suffix}\\n\\n{POSTGRES_TABLE_DEFINITIONS_CAP_REF}\\n\\n{table_definitions}\"\"\"\n",
    "\n",
    "## Append the response format\n",
    "prompt_suffix = f\"\\n\\nRespond only with the SQL query. I need to be able to easily parse the sql query from your response.\"\n",
    "cap_ref_content = f\"\"\"RESPONSE_FORMAT\n",
    "---------\n",
    "<sql query exclusively as raw text>\"\"\"\n",
    "prompt = f\"\"\"{prompt} {prompt_suffix}\\n\\n{cap_ref_content}\"\"\"\n",
    "\n",
    "print(\"\\n\\n-------- PROMPT --------\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the prompt to the AI model\n",
    "NOTE: I had to pin the openai version to 0.28 becuase the 1.0 version has breaking changes.  To keep this example short I did not want to go through the migration guide.\n",
    "https://github.com/openai/openai-python/discussions/742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- RESPONSE --------\n",
      "{\n",
      "  \"id\": \"chatcmpl-1\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1,\n",
      "  \"model\": \"koboldcpp/llama-2-13b-chat.Q5_K_M\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\nSELECT COUNT(*) FROM tblUsers;\"\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = \"llama2\"\n",
    "temperatures = [0.1]\n",
    "max_tokens = 1024\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    temperatures=temperatures,\n",
    "    max_tokens=max_tokens,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\\n-------- RESPONSE --------\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the response and extract the SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- PARSED SQL QUERY --------\n",
      "SELECT COUNT(*) FROM tblUsers;\n"
     ]
    }
   ],
   "source": [
    "# This was taken directly from the example.  I need to review it later.\n",
    "def safe_get(data, dot_chained_keys):\n",
    "    \"\"\"\n",
    "    {'a': {'b': [{'c': 1}]}}\n",
    "    safe_get(data, 'a.b.0.c') -> 1\n",
    "    \"\"\"\n",
    "    keys = dot_chained_keys.split(\".\")\n",
    "    for key in keys:\n",
    "        try:\n",
    "            if isinstance(data, list):\n",
    "                data = data[int(key)]\n",
    "            else:\n",
    "                data = data[key]\n",
    "        except (KeyError, TypeError, IndexError):\n",
    "            return None\n",
    "    return data\n",
    "\n",
    "prompt_response = safe_get(response, \"choices.0.message.content\")\n",
    "\n",
    "sql_query = prompt_response.strip()\n",
    "\n",
    "print(f\"\\n\\n-------- PARSED SQL QUERY --------\")\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the query and print the results\n",
    "Also make sure to close the database connection here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- SQL QUERY RESULTS --------\n",
      "[(150,)]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(sql_query)\n",
    "rows = cursor.fetchall()\n",
    "print(f\"\\n\\n-------- SQL QUERY RESULTS --------\")\n",
    "print(rows)\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
