{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is part 2 of using langchain with local ai models to query an arbitrary database.\n",
    "In this notebook I will try to use langchain with two local models to generate a SQL query, run the query, then format the results into natural language.  I will be using the other unnumbered langchain notebooks as a reference.  I'll also be using some of the logic from the previous autogen notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MSSQL database\n",
    "import pyodbc\n",
    "\n",
    "SERVER = '127.0.0.1'\n",
    "DATABASE = 'TimeBasedCommitments'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = 'BadDefaultPassword!'\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD}'\n",
    "\n",
    "conn = pyodbc.connect(connectionString)\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch database schema to use as context for AI prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table names\n",
    "get_all_tables_stmt = \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES;\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(get_all_tables_stmt)\n",
    "table_names = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Get all table definitions and format them into CREATE TABLE statements\n",
    "get_def_stmt = f\"\"\"\n",
    "SELECT COLUMN_NAME, DATA_TYPE\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE TABLE_NAME = ?\n",
    "\"\"\"\n",
    "\n",
    "definitions = []\n",
    "for table_name in table_names:\n",
    "    cursor.execute(get_def_stmt, (table_name,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    create_table_stmt = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    for row in rows:\n",
    "        create_table_stmt += f\"{row[0]} {row[1]},\\n\"\n",
    "    create_table_stmt = create_table_stmt.rstrip(\",\\n\") + \"\\n);\"\n",
    "    \n",
    "    definitions.append(create_table_stmt)\n",
    "\n",
    "table_definitions = \"\\n\\n\".join(definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model api endpoints\n",
    "This should be updated to use environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DEEPSEEK_CODER_API_BASE = 'http://localhost:5001/v1'\n",
    "MISTRAL_API_BASE = 'http://localhost:5002/api/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build AI prompt used to request a SQL query\n",
    "This prompt works well with https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF.  It uses the recommended prompt template.\n",
    "\n",
    "I had to specify some paramters in the prompt like not using GROUP BY and LIMIT because it was coming up with queries that MSSQL didn't support.\n",
    "\n",
    "I specified not to return just ids because a lot of the query answers were just user ids that wouldnt mean anything to the user.\n",
    "\n",
    "The model seems to respond with markdown wrappers around the SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import KoboldApiLLM\n",
    "\n",
    "deepseek_llm = KoboldApiLLM(endpoint=DEEPSEEK_CODER_API_BASE, temperature=0.1, max_context_length=2048, max_length=1024 )\n",
    "\n",
    "template = \"\"\"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "Create a SQL query that can be used to answer this question: Who has the most specific commitments assigned to them?\n",
    "\n",
    "The query should respond with information other than ids.  For instance instead of returning id return email.\n",
    "\n",
    "If using GROUP BY be sure to include all required columns\n",
    "\n",
    "Do not use the LIMIT keyword because it is not supported in Microsoft SQL\n",
    "\n",
    "Use these TABLE_DEFINITIONS to satisfy the database query.\n",
    "\n",
    "TABLE_DEFINITIONS\n",
    "\n",
    "{table_definitions}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"table_definitions\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=deepseek_llm)\n",
    "\n",
    "question = \"Who has the most specific commitments assigned to them?\"\n",
    "\n",
    "response = llm_chain.run(question = question, table_definitions = table_definitions)\n",
    "\n",
    "sql_query = response.split('```sql')[1].split('```')[0]\n",
    "\n",
    "print(\"#### RESPONSE ####\")\n",
    "print(response)\n",
    "\n",
    "print(\"#### SQL QUERY ####\")\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the SQL query\n",
    "I should wrap this in a loop.  That way if the query fails to run I can generate a new one and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(sql_query)\n",
    "rows = cursor.fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the second model and craft a response based on question, query, and results\n",
    "For this I am using https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = KoboldApiLLM(endpoint=MISTRAL_API_BASE, temperature=0.1, max_context_length=2048, max_length=1024 )\n",
    "\n",
    "template = \"\"\"[INST]\n",
    "Assume this SQL query was run: {sql_query}\n",
    "\n",
    "Assume this was the data returned from the query: {data}\n",
    "\n",
    "Answer this question without mentioning the query: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"sql_query\", \"data\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral_llm)\n",
    "\n",
    "response = llm_chain.run(question = question, sql_query = sql_query, data = rows)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
