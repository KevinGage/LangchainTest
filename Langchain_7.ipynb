{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I will start to work with RAG using Langchain.\n",
    "I will try to load some PDF files using a Langchain loader.  The PDF files are images which will pose a challange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VENV Setup\n",
    "I used chroma as the vector store.  It required python 3.11.  I setup a venv specfic to this notebook.\n",
    "\n",
    "```powershell\n",
    "py -3.11 -m venv winvenv31\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Unstructured Data Loader\n",
    "This loader can be pointed at a folder.  It will attempt to load all files in the folder.\n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory\n",
    "\n",
    "The unstructured data loader uses the python unstructured library.  It's documentation is here\n",
    "https://github.com/Unstructured-IO/unstructured#installing-the-library\n",
    "\n",
    "There are some requirements that must be met to do PDF ocr.\n",
    "\n",
    "Install the following system dependencies if they are not already available on your system. Depending on what document types you're parsing, you may not need all of these.\n",
    "\n",
    "    libmagic-dev (filetype detection)\n",
    "    poppler-utils (images and PDFs)\n",
    "    tesseract-ocr (images and PDFs, install tesseract-lang for additional language support)\n",
    "    libreoffice (MS Office docs)\n",
    "    pandoc (EPUBs, RTFs and Open Office docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing poppler\n",
    "Download the windows poppler binaries from here\n",
    "https://github.com/oschwartz10612/poppler-windows/releases\n",
    "\n",
    "Add the bin folder to the windows path as seen here\n",
    "https://stackoverflow.com/questions/73669337/how-to-how-to-install-poppler-from-the-tar-file-downloaded-from-poppler-officia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing tesseract\n",
    "Notes on windows install here\n",
    "https://tesseract-ocr.github.io/tessdoc/Installation.html\n",
    "\n",
    "Download from here\n",
    "https://github.com/UB-Mannheim/tesseract/wiki\n",
    "\n",
    "Run installer\n",
    "\n",
    "Add C:\\Program Files\\Tesseract-OCR to the windows path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install cmake\n",
    "%pip install pytesseract\n",
    "%pip install unstructured\n",
    "%pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PDF files\n",
    "The directory loader can import a whole folder of misc files.  In this example I am loading a folder of just PDF files to test OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kgage\\source\\repos\\Personal\\LangchainTest\\winvenv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('D:\\\\Temp\\\\OldCrsToTestAiRAG', glob=\"**/*.pdf\", use_multithreading=True)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "Next the loaded docs are split into chunks.  Whole docs wont fit into a context window.  The chunks contain some overlap so context is kept between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "Next a vectory store (vector DB) must be created to store the chunk vectors.  There are a variety of options.  For this notebook I will use chromadb as decribed here\n",
    "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "Chroma requires python 3.11.  It did not work with 3.12.  Issue to track here\n",
    "https://github.com/chroma-core/chroma/issues/1410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb\n",
    "%pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fetching something from vectorstore\n",
    "question = \"Locked high radiation barrier inconsistent with \"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)\n",
    "\n",
    "# note documents do contain metadata, like file name/path\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the llm\n",
    "In this case I'll use KoboldCPP \n",
    "I tried running llama-2-13b.Q6_K but results weren't impressive.\n",
    "using orca-2-13b.Q5_K_M gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import KoboldApiLLM\n",
    "\n",
    "llm = KoboldApiLLM(endpoint='http://localhost:5001/v1', temperature=0.1, max_context_length=4096 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a query with the docs embedded\n",
    "This is just a test to see if the llm and docs are both working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "question = \"What happened when flushing RM-1664?\"\n",
    "# note documents do contain metadata, like file name/path\n",
    "docs = vectorstore.similarity_search(question)\n",
    "result = llm_chain(docs)\n",
    "\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to include sources\n",
    "https://python.langchain.com/docs/use_cases/question_answering/#adding-sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\I3110_04-66.pdf'},\n",
       "  {'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\04-66.pdf'},\n",
       "  {'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\04-62.pdf'},\n",
       "  {'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\04-93.pdf'},\n",
       "  {'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\04-68.pdf'},\n",
       "  {'source': 'D:\\\\Temp\\\\OldCrsToTestAiRAG\\\\04-89.pdf'}],\n",
       " 'answer': 'When the backhoe ran over the FSS flags at Foxbird Island, immediate actions included instructing workers to suspend activities that would further disturb the FSS markings, notifying Manafort management, and notifying the FSS and Remediation Superintendent.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# This was the example template from the docs\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# Use three sentences maximum and keep the answer as concise as possible.\n",
    "# Always say \"thanks for asking!\" at the end of the answer.\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "\n",
    "template = \"\"\"Use the following pieces of CONTEXT to ANSWER the QUESTION at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "CONTEXT\n",
    "{context}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "ANSWER\n",
    "\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"What actions were taken when the backhoe ran over the flags at Foxbird Island?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
